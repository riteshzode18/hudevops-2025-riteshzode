## DataOps & MLOps

---

### 1. **Question:** End-to-End ETL Pipeline with AWS

_As a DevOps Engineer, youâ€™re tasked with setting up an end-to-end ETL pipeline using AWS services._

#### **S3 Bucket Structure**
- `athena_results/`  
  _Contains CSV and Parquet files generated as a result of Athena queries (should be automated)._
- `data/`
  - `customers_database/`
    - `customers_csv/`
    - `customers_parquet/` _(after transformation)_
- `scripts/`  
  _Scripts generated by Glue Visual jobs should automatically populate here._
- `temp-dir/`

#### **Pipeline Requirements**
- Ingest raw CSV data.
- Crawl data and make it queryable in Athena.
- Transform data:  
  - Replace first name with `hu25`.
  - Mask sensitive fields using a Glue Visual job.
- Write transformed output as Parquet.
- Store the ETL script in the appropriate folder.
- Design, implement, and validate this pipeline using AWS Glue, IAM, S3, and Athena.

---

### 2. **Question:** CI/CD Pipeline for AWS Glue ETL Automation

_Set up a comprehensive CI/CD pipeline using AWS services to automate the processing of ETL scripts and job configurations, manage AWS Glue jobs, and ensure the following workflow (make use of the Glue job created in Question 1):_

#### a. **Triggering the Pipeline**
- Automatically trigger the pipeline when there are new commits to specific directories (ETL scripts and job configurations) in an AWS CodeCommit repository.

#### b. **Processing Committed Files**
- Use an AWS Lambda function to:
  - Detect recently committed files in the specified directories of the CodeCommit repository.
  - Fetch the committed ETL scripts and job configurations from CodeCommit.
  - Create or update AWS Glue jobs based on the job configurations.
  - Start the AWS Glue jobs and monitor their execution status.

#### c. **Managing AWS Glue Jobs**
- Load job configurations from the committed files.
- Derive job names from the configuration file names.
- Check if the corresponding AWS Glue jobs already exist.
- Create new AWS Glue jobs if they do not exist, or update existing ones with the new configurations.
- Start the AWS Glue jobs and monitor their execution status.

#### d. **Handling Pipeline Success and Failure**
- Signal success to AWS CodePipeline if all steps are completed successfully.
- Handle errors gracefully and signal failure to AWS CodePipeline if any step fails.

---

### 3. **Question:** Advanced Apache Airflow DAG for Customer Data Processing

_As a DevOps Engineer for a data engineering team, you're tasked with building an advanced Apache Airflow DAG locally. This DAG will process customer data to enhance analytics workflows with the following functionality:_

1. **Pull a CSV file** from a local directory, simulating an external data source. Track metadata like file size and ingestion times.
2. **Validate** that the CSV includes required columns: `id`, `name`, `email`, `timestamp`.
3. **Remove** null entries and duplicates.
4. **Normalize** names to Title Case, ensure email formats are lowercase, and mask email components for privacy.
5. **Use a shell script** to detect differences by flagging future timestamps.
6. **Save the cleaned file** to a new CSV, backup the original with a timestamp.
7. **Use a cron job** to execute the Airflow DAG at regular intervals.

---

### 4. **Question:** Local Apache Airflow DAG for ML Workflow

_As a DevOps Engineer, your role involves setting up and managing the infrastructure and workflows that support the Machine Learning team's operations. This specific task requires you to install Apache Airflow locally and create a Directed Acyclic Graph (DAG) workflow to automate the process of pulling a dataset and training a model._

#### i. **Write a DAG Workflow**
- **Define the DAG:**  
  Create a new DAG file in your local Airflow directory. This file will define the workflow, including tasks and dependencies.
- **Pull Dataset:**  
  Write a task within the DAG to download the dataset from Kaggle using the provided URL:  
  [https://www.kaggle.com/datasets/saurabh00007/iriscsv](https://www.kaggle.com/datasets/saurabh00007/iriscsv)
- **Train Model:**  
  Add a task to the DAG for training a machine learning model using the downloaded dataset. This could involve using libraries like scikit-learn or TensorFlow, depending on the model requirements.

#### ii. **Manage DAGs Locally**
- **Persist Data:**  
  Ensure that the data pulled from Kaggle is stored in a local DAG folder. This involves setting up file paths and storage mechanisms within your DAG tasks.